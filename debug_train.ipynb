{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import grain\n",
    "from math import ceil\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import NamedSharding, PartitionSpec as P\n",
    "from flax import nnx, struct\n",
    "import optax\n",
    "from sws import run\n",
    "import wandb\n",
    "import orbax.checkpoint as ocp\n",
    "from datetime import datetime\n",
    "from dataset import get_data_loader\n",
    "from modelling.model import Model\n",
    "from modelling.optimizers import adamw_atan2, sign_sgdw\n",
    "from utils import MetricLogger\n",
    "from evaluate import evaluate\n",
    "\n",
    " # TODO: fix crop function to also crop top/left based on bottom/right border. tmp solution is translate=False on val set.\n",
    " \n",
    "def main(cfg):\n",
    "\n",
    "    if cfg.parallel.n_devices > 1:\n",
    "        jax.distributed.initialize()\n",
    "\n",
    "    key = jax.random.key(cfg.seed)\n",
    "    \n",
    "    model = Model(**cfg.model.to_dict(), rngs=nnx.Rngs(key))\n",
    "\n",
    "    opt_fn = adamw_atan2 if cfg.optim.use_atan2 else optax.adamw\n",
    "    tx = optax.partition(\n",
    "        {\n",
    "            \"embed\": sign_sgdw(\n",
    "                optax.warmup_constant_schedule(**cfg.embed_schedule.to_dict()), cfg.optim.weight_decay\n",
    "            ),\n",
    "            \"other\": opt_fn(\n",
    "                optax.warmup_constant_schedule(**cfg.other_schedule.to_dict()),\n",
    "                b1=cfg.optim.b1,\n",
    "                b2=cfg.optim.b2,\n",
    "                weight_decay=cfg.optim.weight_decay,\n",
    "            )\n",
    "        },\n",
    "        lambda state: jax.tree.map_with_path(lambda path, _: \"embed\" if path[0].key == \"puzzle_emb\" else \"other\", state)\n",
    "    )\n",
    "    optimizer = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "    \n",
    "    shard_data = lambda data: data\n",
    "    if cfg.parallel.n_devices > 1:\n",
    "        mesh = jax.make_mesh((cfg.parallel.n_devices,), (\"data\",))\n",
    "        jax.set_mesh(mesh)\n",
    "\n",
    "        if jax.process_index() == 0:\n",
    "            print(f\"{mesh=}\")\n",
    "\n",
    "        repl_sharding = NamedSharding(mesh, P())\n",
    "        data_sharding = NamedSharding(mesh, P(\"data\",))\n",
    "\n",
    "        _, model_state = nnx.split(model)\n",
    "        sharded_model_state = jax.lax.with_sharding_constraint(model_state, repl_sharding)\n",
    "        # fsdp on puzzle embs\n",
    "        sharded_model_state.puzzle_emb = jax.lax.with_sharding_constraint(sharded_model_state.puzzle_emb, data_sharding)\n",
    "        nnx.update(model, sharded_model_state)\n",
    "        \n",
    "        _, optimizer_state = nnx.split(optimizer)\n",
    "        sharded_optimizer_state = jax.lax.with_sharding_constraint(optimizer_state, repl_sharding)\n",
    "        nnx.update(optimizer, sharded_optimizer_state)\n",
    "\n",
    "        shard_data = lambda data: jax.tree.map(lambda x: jax.make_array_from_process_local_data(data_sharding, x), data)\n",
    "\n",
    "\n",
    "    @struct.dataclass\n",
    "    class Carry:\n",
    "        z: jax.Array\n",
    "        y: jax.Array\n",
    "        x_input: jax.Array\n",
    "        aug_puzzle_idx: jax.Array\n",
    "        y_true: jax.Array\n",
    "        step: jax.Array\n",
    "        halted: jax.Array\n",
    "    \n",
    "    \n",
    "    def init_carry(batch, z_init, y_init):\n",
    "        \"\"\"initialize the carry with the initial data\"\"\"\n",
    "        batch_size = batch['x'].shape[0]\n",
    "        hidden_dim = z_init.shape[-1]\n",
    "        if cfg.model.vision_mode:\n",
    "            seq_len = cfg.model.input_size // cfg.model.patch_size * cfg.model.input_size // cfg.model.patch_size\n",
    "        else:\n",
    "            seq_len = 900\n",
    "        seq_len = seq_len + cfg.model.puzzle_emb_len\n",
    "        z_init = jnp.broadcast_to(z_init, (batch_size, seq_len, hidden_dim))\n",
    "        y_init = jnp.broadcast_to(y_init, (batch_size, seq_len, hidden_dim))\n",
    "        if cfg.parallel.n_devices > 1:\n",
    "            z_init = jax.device_put(z_init, data_sharding)\n",
    "            y_init = jax.device_put(y_init, data_sharding)\n",
    "        return Carry(\n",
    "            z=z_init,                                         # (batch_size, seq_len, hidden_dim)\n",
    "            y=y_init,                                         # (batch_size, seq_len, hidden_dim)\n",
    "            x_input=batch['x'],                               # (batch_size, 900)\n",
    "            aug_puzzle_idx=batch['aug_puzzle_idx'],           # (batch_size,)\n",
    "            y_true=batch['y'],                                # (batch_size, 900)\n",
    "            step=jnp.zeros((batch_size, ), dtype=jnp.int32),  # (batch_size,)\n",
    "            halted=jnp.zeros((batch_size, ), dtype=jnp.bool_) # (batch_size,)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def pre_update_carry(carry, batch, z_init, y_init):\n",
    "        \"\"\"update the carry with new data from batch (if halted)\"\"\"\n",
    "        return Carry(\n",
    "            z=jnp.where(carry.halted[..., jnp.newaxis, jnp.newaxis], z_init[jnp.newaxis, jnp.newaxis, ...], carry.z),\n",
    "            y=jnp.where(carry.halted[..., jnp.newaxis, jnp.newaxis], y_init[jnp.newaxis, jnp.newaxis, ...], carry.y),\n",
    "            x_input=jnp.where(carry.halted[..., jnp.newaxis], batch['x'], carry.x_input),\n",
    "            aug_puzzle_idx=jnp.where(carry.halted[..., jnp.newaxis], batch['aug_puzzle_idx'], carry.aug_puzzle_idx),\n",
    "            y_true=jnp.where(carry.halted[..., jnp.newaxis], batch['y'], carry.y_true),\n",
    "            step=jnp.where(carry.halted, 0, carry.step),\n",
    "            halted=jnp.where(carry.halted, False, carry.halted),\n",
    "        )\n",
    "    \n",
    "\n",
    "    def post_update_carry(carry, q_logits, z, y, N_supervision, halt_explore_prob, key):\n",
    "        \"\"\"update the halt flag if step >= N_supervision or q_logits > 0\"\"\"\n",
    "        step = carry.step + 1\n",
    "        halted = step >= N_supervision\n",
    "        if cfg.recursion.act:\n",
    "            halted = halted | (q_logits.reshape(-1) > 0)\n",
    "        if halt_explore_prob > 0:\n",
    "            key, subkey, subkey2 = jax.random.split(key, 3)\n",
    "            min_halt_steps = (\n",
    "                (jax.random.uniform(subkey, halted.shape) < halt_explore_prob)\n",
    "                * jax.random.randint(subkey2, step.shape, minval=2, maxval=N_supervision + 1)\n",
    "            )\n",
    "            halted = halted & (step >= min_halt_steps)\n",
    "        return Carry(\n",
    "            z=z,\n",
    "            y=y,\n",
    "            x_input=carry.x_input,\n",
    "            aug_puzzle_idx=carry.aug_puzzle_idx,\n",
    "            y_true=carry.y_true,\n",
    "            step=step,\n",
    "            halted=halted,\n",
    "        ), key\n",
    "\n",
    "    \n",
    "    def stablemax_cross_entropy_with_integer_labels(logits, labels, eps=1e-30):\n",
    "        pos = jnp.log(jnp.maximum(logits, 0.) + 1.0 + eps)\n",
    "        neg = -jnp.log(jnp.maximum(1.0 - logits, eps))\n",
    "\n",
    "        s_logits = jnp.where(logits >= 0, pos, neg)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(s_logits, labels)\n",
    "\n",
    "\n",
    "    def latent_recursion(model, x, y, z, n):\n",
    "        def latent_recursion_body(z, _):\n",
    "            return model(x, y, z), None\n",
    "        with jax.named_scope(\"latent_recursion_scan\"):\n",
    "            z, _ = jax.lax.scan(latent_recursion_body, z, None, length=n, unroll=True)\n",
    "        with jax.named_scope(\"latent_recursion_last\"):\n",
    "            y = model(y, z)\n",
    "        return y, z\n",
    "\n",
    "\n",
    "    def deep_recursion(model, x, y, z, n, T):\n",
    "        def deep_recursion_body(carry, _):\n",
    "            y, z = carry\n",
    "            y, z = latent_recursion(model, x, y, z, n)\n",
    "            return (y, z), None\n",
    "        with jax.named_scope(\"deep_recursion_scan\"):\n",
    "            (y, z), _ = jax.lax.scan(deep_recursion_body, (y, z), None, length=T, unroll=True)\n",
    "        return y, z\n",
    "\n",
    "\n",
    "    def loss_fn(model, x_input, aug_puzzle_idx, y, z, y_true, n):\n",
    "        # forward pass\n",
    "        x = model.input_embedding(x_input, aug_puzzle_idx)\n",
    "        y, z = latent_recursion(model, x, y, z, n)\n",
    "        y_logits, q_logits = model.output_head(y), model.q_head(y)\n",
    "        y_preds = jnp.argmax(y_logits, axis=-1)\n",
    "        # compute losses\n",
    "        y_loss = stablemax_cross_entropy_with_integer_labels(\n",
    "            y_logits.reshape(-1, y_logits.shape[-1]).astype(jnp.float32),\n",
    "            y_true.reshape(-1)\n",
    "        ).mean(where=y_true.reshape(-1) < 11)\n",
    "        if cfg.recursion.act:\n",
    "            # TODO: only compute for halted ?\n",
    "            q_loss = optax.sigmoid_binary_cross_entropy(\n",
    "                q_logits.reshape(-1).astype(jnp.float32),\n",
    "                (y_preds == y_true).all(axis=-1, where=y_true < 11)\n",
    "            ).mean()\n",
    "        else:\n",
    "            q_loss = 0\n",
    "        loss = y_loss + 0.5 * q_loss # 0.5* why?\n",
    "        return loss, (y, z, y_loss, q_loss, y_preds, q_logits)\n",
    "    \n",
    "    \n",
    "    @nnx.jit(static_argnames=[\"N_supervision\", \"n\", \"T\", \"halt_explore_prob\"])\n",
    "    def train_step(model, optimizer, carry, batch, y_init, z_init, N_supervision, n, T, halt_explore_prob, ema_model, key):\n",
    "        # update carry (if halted, update with init and batch)\n",
    "        carry = pre_update_carry(carry, batch, z_init, y_init)\n",
    "        # 1 N_supervision step\n",
    "        x = model.input_embedding(carry.x_input, carry.aug_puzzle_idx)\n",
    "        # deep recursion loop (no grads)\n",
    "        z, y = carry.z, carry.y\n",
    "        y, z = deep_recursion(model, x, y, z, n, T-1)\n",
    "        # 1-step approx BPTT\n",
    "        grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, (y, z, y_loss, q_loss, y_preds, q_logits)), grads = grad_fn(\n",
    "            model, carry.x_input, carry.aug_puzzle_idx, y, z, carry.y_true, n\n",
    "        )\n",
    "        optimizer.update(model, grads)\n",
    "\n",
    "        # update halt flag\n",
    "        carry, key = post_update_carry(carry, q_logits, z, y, N_supervision, halt_explore_prob, key)\n",
    "\n",
    "        # compute metrics (11 = padding)\n",
    "        cell_correct = y_preds == carry.y_true # (batch_size, 900)\n",
    "        puzzle_correct = cell_correct.all(axis=-1, where=carry.y_true < 11)\n",
    "        cell_acc = cell_correct.mean(where=(carry.y_true < 11) & (carry.halted[..., jnp.newaxis]))\n",
    "        puzzle_acc = puzzle_correct.mean(where=carry.halted)\n",
    "        metrics = {\n",
    "            \"loss\": loss,\n",
    "            \"y_loss\": y_loss,\n",
    "            \"q_loss\": q_loss,\n",
    "            \"cell_acc\": cell_acc,\n",
    "            \"puzzle_acc\": puzzle_acc,\n",
    "            \"y_max\": jnp.max(jnp.abs(y)),\n",
    "            \"z_max\": jnp.max(jnp.abs(z)),\n",
    "            \"y_norm\": jnp.sqrt(jnp.mean(y**2)),\n",
    "            \"z_norm\": jnp.sqrt(jnp.mean(z**2)),\n",
    "            \"n_supervision_steps\": carry.step.mean(where=carry.halted),\n",
    "        }\n",
    "        if cfg.recursion.act:\n",
    "            q_acc = ((q_logits.reshape(-1) > 0) == puzzle_correct).mean(where=carry.halted)\n",
    "            metrics[\"q_acc\"] = q_acc\n",
    "\n",
    "        if ema_model is not None:\n",
    "            new_ema_state = jax.tree.map(\n",
    "                lambda new, old: (\n",
    "                    None if new is None else (1.0 - cfg.ema_weight) * new + cfg.ema_weight * old\n",
    "                ),\n",
    "                nnx.state(model),\n",
    "                nnx.state(ema_model),\n",
    "                is_leaf=lambda x: x is None,\n",
    "            )\n",
    "            nnx.update(ema_model, new_ema_state)\n",
    "\n",
    "        return carry, metrics, key\n",
    "    \n",
    "    # init logging \n",
    "    if jax.process_index() == 0:\n",
    "        if cfg.wandb:\n",
    "            wandb.init(project=\"arc\", entity=\"jackpenn\", config=cfg.to_dict())\n",
    "            train_logger = MetricLogger(cfg.data.train_batch_size, prefix=\"train\", buffer=True, wandb=wandb)\n",
    "            val_logger = MetricLogger(cfg.data.eval_batch_size, prefix=\"val\", buffer=False, wandb=wandb)\n",
    "        else:\n",
    "            train_logger = MetricLogger(cfg.data.train_batch_size, prefix=\"train\", buffer=True, wandb=None)\n",
    "            val_logger = MetricLogger(cfg.data.eval_batch_size, prefix=\"val\", buffer=False, wandb=None)\n",
    "\n",
    "    # count params\n",
    "    _, params = nnx.split(model, nnx.Param)\n",
    "    num_params = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "    puzzle_emb_params = model.puzzle_emb.embedding.size\n",
    "    num_params -= puzzle_emb_params\n",
    "    print(f\"{num_params=}, {puzzle_emb_params=}\")\n",
    "    del params\n",
    "    \n",
    "    # init latents\n",
    "    key, y_key, z_key = jax.random.split(key, 3)\n",
    "    initializer = jax.nn.initializers.truncated_normal(stddev=1.0)\n",
    "    y_init = initializer(y_key, (cfg.model.hidden_dim,), jnp.bfloat16)\n",
    "    z_init = initializer(z_key, (cfg.model.hidden_dim,), jnp.bfloat16) \n",
    "    \n",
    "\n",
    "    # init data loader\n",
    "    train_data_loader = get_data_loader(\n",
    "        cfg.data.data_dir + \"/train.jsonl\",\n",
    "        cfg.data.train_batch_size,\n",
    "        translate=cfg.data.translate,\n",
    "        max_grid_size=cfg.data.max_grid_size,\n",
    "        repeat=True,\n",
    "        drop_remainder=True,\n",
    "        shard_by_jax_process=True\n",
    "    )\n",
    "    val_data_loader_factory = lambda: get_data_loader(\n",
    "        cfg.data.data_dir + \"/test.jsonl\",\n",
    "        cfg.data.eval_batch_size,\n",
    "        translate=False,\n",
    "        max_grid_size=cfg.data.max_grid_size,\n",
    "        repeat=False,\n",
    "        drop_remainder=True,\n",
    "        shard_by_jax_process=True\n",
    "    ) # tmp drop remainder because of sharding ( so eval on n lik 99% subset)\n",
    "\n",
    "    ckpt_dir = os.path.join(cfg.ckpt_dir, \"checkpoints\")\n",
    "    if jax.process_index() == 0:\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    ckpt_options = ocp.CheckpointManagerOptions(max_to_keep=1, cleanup_tmp_directories=True)\n",
    "    ckpt_mngr = ocp.CheckpointManager(ckpt_dir, options=ckpt_options)\n",
    "\n",
    "    # init profiler\n",
    "    profiler_options = jax.profiler.ProfileOptions()\n",
    "    profiler_options.host_tracer_level = 3\n",
    "    profile_dir = f\"profile_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "\n",
    "    train_iter = iter(train_data_loader)\n",
    "    if cfg.restore_from_checkpoint:\n",
    "        abstract_model_state = nnx.state(nnx.eval_shape(lambda: model))\n",
    "        abstract_optim_state = nnx.state(nnx.eval_shape(lambda: optimizer))\n",
    "        restore_args = dict(\n",
    "            z_init=ocp.args.ArrayRestore(z_init),\n",
    "            y_init=ocp.args.ArrayRestore(y_init),\n",
    "            model_state=ocp.args.StandardRestore(abstract_model_state),\n",
    "            optim_state=ocp.args.StandardRestore(abstract_optim_state),\n",
    "            # data_loader=grain.checkpoint.CheckpointRestore(train_iter),\n",
    "        )\n",
    "        if cfg.use_ema:\n",
    "            restore_args[\"ema_model\"] = ocp.args.StandardRestore(abstract_model_state)\n",
    "        restored = ckpt_mngr.restore(ckpt_mngr.latest_step(), args=ocp.args.Composite(**restore_args))\n",
    "        step = ckpt_mngr.latest_step() + 1\n",
    "        nnx.update(model, restored.model_state)\n",
    "        nnx.update(optimizer, restored.optim_state)\n",
    "        if cfg.use_ema:\n",
    "            ema_model = nnx.clone(model)\n",
    "            nnx.update(ema_model, restored.ema_model)\n",
    "\n",
    "        z_init = restored.z_init\n",
    "        y_init = restored.y_init\n",
    "        # train_iter = restored.data_loader\n",
    "    else:\n",
    "        step = 0\n",
    "        if cfg.use_ema:\n",
    "            ema_model = nnx.clone(model)\n",
    "\n",
    "    carry = init_carry(shard_data(next(train_iter)), z_init, y_init)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    while step < cfg.max_steps + 1:\n",
    "        batch = shard_data(next(train_iter))\n",
    "\n",
    "        if jax.process_index() == 0 and step == 10: \n",
    "            jax.profiler.start_trace(profile_dir, profiler_options=profiler_options)\n",
    "        with jax.profiler.StepTraceAnnotation(\"train_step\", step_num=step):\n",
    "            carry, metrics, key = train_step(\n",
    "                model, optimizer, carry, batch, y_init, z_init,\n",
    "                cfg.recursion.N_supervision, cfg.recursion.n, cfg.recursion.T,\n",
    "                cfg.recursion.halt_explore_prob,\n",
    "                ema_model if cfg.use_ema else None,\n",
    "                key\n",
    "            )\n",
    "        if jax.process_index() == 0 and step == 15:\n",
    "            jax.profiler.stop_trace()\n",
    "            if cfg.wandb:\n",
    "                wandb.log_artifact(f\"{os.getcwd()}/{profile_dir}/\", name=f\"run_{wandb.run.id}_profile\", type=\"profile\")\n",
    "\n",
    "        step_time = time.perf_counter() - t0\n",
    "        t0 = time.perf_counter()\n",
    "        \n",
    "        if jax.process_index() == 0:\n",
    "            train_logger.log({**metrics, \"step_time\": step_time, \"step\": step})\n",
    "\n",
    "        if step > 0 and step % cfg.log_every == 0:\n",
    "            args = dict(\n",
    "                z_init=ocp.args.ArraySave(z_init),\n",
    "                y_init=ocp.args.ArraySave(y_init),\n",
    "                model_state=ocp.args.StandardSave(nnx.state(model)),\n",
    "                optim_state=ocp.args.StandardSave(nnx.state(optimizer)),\n",
    "                # data_loader=grain.checkpoint.CheckpointSave(train_iter),\n",
    "            )\n",
    "            if cfg.use_ema:\n",
    "                args[\"ema_model\"] = ocp.args.StandardSave(nnx.state(ema_model))\n",
    "            ckpt_mngr.save(step, args=ocp.args.Composite(**args))\n",
    "            ckpt_mngr.wait_until_finished()\n",
    "            if jax.process_index() == 0 and cfg.wandb:\n",
    "                wandb.log_model(f\"{ckpt_dir}/{step}\", name=f\"{wandb.run.id}_model\", aliases=[f\"step_{step}\"])    \n",
    "\n",
    "        if step > 0 and step % cfg.eval.eval_every == 0:\n",
    "            val_metrics = evaluate(\n",
    "                ema_model if cfg.use_ema else model,\n",
    "                val_data_loader_factory, y_init, z_init,\n",
    "                cfg.recursion.N_supervision, cfg.recursion.n, cfg.recursion.T,\n",
    "                cfg.eval.pass_ks, shard_data, cfg.data.eval_batch_size\n",
    "            )\n",
    "            if jax.process_index() == 0:\n",
    "                val_logger.log({**val_metrics, \"step_time\": step_time, \"step\": step})\n",
    "        \n",
    "        step += 1\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
